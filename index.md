# Practical Machine Learning - Course Project
Katie Martins  
10/11/2017  

## Introduction

Popular devices like Jawbone Up, Nike FuelBand, and Fitbit allow for the collection of a large amount of data related to personal activity. The data collected from these devices is often used to quantify how much of a particular activity people are doing, but it is less often used to quantify how well they are doing it. 

In this project, we will build a machine learning model to perform quantitative activity recognition of weight lifting exercises. The dataset used in this project comes from http://groupware.les.inf.puc-rio.br/har. More information about the experiment can be found at that website.

Six participants were asked to perform one set of 10 repetitions of the unilateral dumbell biceps curl in 5 different manners:

* Class A: Exercise performed correctly
* Class B: Throwing the elbows to the front
* Class C: Lifting the dumbell only halfway
* Class D: Lowering the dumbell only halfway
* Class E: Throwing the hips to the front

The dataset contains observations comprised of measurements for each participant performing the exercise in each of the manners above, along with class labels identifying the manner in which the exercise was performed.

## Downloading Data

First, we load the libraries we will need for the analysis.


```r
library(dplyr)
library(ggplot2)
library(caret)
library(readr)
library(knitr)
library(Hmisc)
library(rattle)
```

Then we need to download the data and read it into R. 


```r
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

download.file(trainURL, "training.csv")
download.file(testURL, "testing.csv")

training <- read_csv("training.csv", na = c("NA", ""))
testing <- read_csv("testing.csv", na = c("NA", ""))
```

## Data Processing


```r
head(names(training), 15)
```

```
##  [1] "X1"                   "user_name"            "raw_timestamp_part_1"
##  [4] "raw_timestamp_part_2" "cvtd_timestamp"       "new_window"          
##  [7] "num_window"           "roll_belt"            "pitch_belt"          
## [10] "yaw_belt"             "total_accel_belt"     "kurtosis_roll_belt"  
## [13] "kurtosis_picth_belt"  "kurtosis_yaw_belt"    "skewness_roll_belt"
```

The first seven columns contain identifying information that will not be used to fit our model, so we will remove them from the data. 


```r
train <- training[, -(1:7)]
test <- testing[, -(1:7)]
```


```r
percent_na <- apply(train, 2, function(x) mean(is.na(x)))
na_cut <- cut2(percent_na, cuts = seq(0, 1, by = 0.1))
table(na_cut)
```

```
## na_cut
## [0.0,0.1)       0.1       0.2       0.3       0.4       0.5       0.6 
##        53         0         0         0         0         0         0 
##       0.7       0.8 [0.9,1.0] 
##         0         0       100
```
Of the 153 variables in our dataset, 100 of them contain over 90% missing values. Let's remove these variables from consideration. (These actually constitute aggregate statistics generated by the research team in their window-based approach to modeling the data, so, missing or not, it wouldn't make sense for us to include them in our model.)

```r
na_cols <- which(percent_na > 0.9)
train <- train[,-na_cols]
test <- test[,-na_cols]
dim(train)
```

```
## [1] 19622    53
```
We are now down to 53 variables. Let's check whether there are any remaining missing values in these columns.

```r
nas <- sapply(train, function(x) sum(is.na(x)))
subset(nas, nas > 0)
```

```
## magnet_dumbbell_z  magnet_forearm_y  magnet_forearm_z 
##                 1                 1                 1
```

Three of the remaining variables have one missing value. We will use median imputation to take care of those.


```r
preProc <- preProcess(train, method = "medianImpute")
train_imp <- predict(preProc, train)
```

We have already reduced the number of variables substantially by removing the columns that are predominantly NA, but let's check to see if any of the remaining columns are near zero variance.


```r
nzv_cols <- nearZeroVar(train)
length(nzv_cols)
```

```
## [1] 0
```

Because `length(nzv_cols)` is 0, none of our remaining variables can be excluded on the basis of near zero variance.

## Decision Tree Model

Now that we have cleaned up our data set and discarded some unnecessary variables, we can start fitting models. 

I am using 5-fold cross validation to estimate out of sample error. Additionally, for the purposes of this course project, I am holding out 20% of the data to use as a validation set. All of the data in `mytrain` will be used in fitting the model. The data in `myval` will only be used as new data for prediction.


```r
set.seed(21312)
inTrain <- createDataPartition(train$classe, p = 0.8, list = FALSE)
mytrain <- train_imp[inTrain, ]
myval <- train_imp[-inTrain, ]
dim(mytrain)
```

```
## [1] 15699    53
```

```r
dim(myval)
```

```
## [1] 3923   53
```

Let's build a simple decision tree using `rpart` with the default tuning parameters as a baseline model.


```r
set.seed(21312)
ctrl <- trainControl(method = "cv", number = 5)
rpart_fit <- train(classe ~ ., 
                   data = mytrain, 
                   method="rpart", 
                   trControl = ctrl)
rpart_fit
```

```
## CART 
## 
## 15699 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 12558, 12561, 12560, 12558, 12559 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa     
##   0.03631509  0.5077452  0.35768778
##   0.05957573  0.4158050  0.20907277
##   0.11517579  0.3489998  0.09877358
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.03631509.
```

The accuracy estimate from this model is about 50.7%. We can use the `rattle` package to visualize our simple decision tree and see that the low accuracy is not surprising - only four variables were used in the final tree.


```r
print(fancyRpartPlot(rpart_fit$finalModel))
```

![](index_files/figure-html/rpartplot-1.png)<!-- -->

```
## NULL
```

Let's check how the baseline model does on the validation set and compare the result to the accuracy estimate from the cross validation procedure.


```r
rpart_pred <- predict(rpart_fit, myval)
confusionMatrix(rpart_pred, myval$classe)
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1014  317  300  293   99
##          B   25  268   25  119   89
##          C   75  174  359  231  208
##          D    0    0    0    0    0
##          E    2    0    0    0  325
## 
## Overall Statistics
##                                           
##                Accuracy : 0.5011          
##                  95% CI : (0.4854, 0.5169)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.3483          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9086  0.35310  0.52485   0.0000  0.45076
## Specificity            0.6405  0.91846  0.78759   1.0000  0.99938
## Pos Pred Value         0.5012  0.50951  0.34288      NaN  0.99388
## Neg Pred Value         0.9463  0.85546  0.88700   0.8361  0.88988
## Prevalence             0.2845  0.19347  0.17436   0.1639  0.18379
## Detection Rate         0.2585  0.06832  0.09151   0.0000  0.08284
## Detection Prevalence   0.5157  0.13408  0.26689   0.0000  0.08335
## Balanced Accuracy      0.7746  0.63578  0.65622   0.5000  0.72507
```

The 50% accuracy on the validation set is in line with the out of sample error estimate from the cross validation procedure. Looking at the confusion matrix, we can see that this model does a decent job of predicting Class A, but does not predict that any of the observations are in Class D at all.

Before moving to more complex models, let's explore `rpart` further by considering tuning. The tuning parameter is the complexity parameter `cp`, and lower values of `cp` will result in more complex trees (i.e. more nodes = more variables included in the model). Let's try `cp = 0.01`.


```r
grid <- data.frame(cp = seq(0.01, 0.1, by = 0.01))
ctrl <- trainControl(method = "cv", number = 5)
rpart_cp_fit <- train(classe ~ ., 
                        data = mytrain, 
                        method="rpart", 
                        trControl = ctrl,
                        tuneGrid = grid)
rpart_cp_fit
```

```
## CART 
## 
## 15699 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 12560, 12558, 12560, 12559, 12559 
## Resampling results across tuning parameters:
## 
##   cp    Accuracy   Kappa    
##   0.01  0.7395362  0.6703047
##   0.02  0.6434802  0.5520745
##   0.03  0.5406071  0.4018268
##   0.04  0.4942345  0.3395964
##   0.05  0.4942345  0.3395964
##   0.06  0.4155808  0.2082395
##   0.07  0.3663294  0.1253103
##   0.08  0.3663294  0.1253103
##   0.09  0.3663294  0.1253103
##   0.10  0.3663294  0.1253103
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.01.
```

This gives an accuracy estimate of about 74%. The tree includes many more variables than the tree with the default parameters.


```r
fancyRpartPlot(rpart_cp_fit$finalModel)
```

```
## Warning: labs do not fit even at cex 0.15, there may be some overplotting
```

![](index_files/figure-html/rpart_tune_rattle-1.png)<!-- -->

Just for fun, let's make the tree even more complex and look at the effect of `cp` on accuracy as we go lower.


```r
grid <- data.frame(cp = seq(0.0005, 0.01, by = 0.001))
ctrl <- trainControl(method = "cv", number = 5)
rpart_tune_fit <- train(classe ~ ., 
                        data = mytrain, 
                        method="rpart", 
                        trControl = ctrl,
                        tuneGrid = grid)
rpart_tune_fit
```

```
## CART 
## 
## 15699 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 12559, 12559, 12559, 12560, 12559 
## Resampling results across tuning parameters:
## 
##   cp      Accuracy   Kappa    
##   0.0005  0.9209508  0.9000074
##   0.0015  0.8783364  0.8460253
##   0.0025  0.8454690  0.8043446
##   0.0035  0.8175054  0.7688198
##   0.0045  0.7983323  0.7446800
##   0.0055  0.7891602  0.7329915
##   0.0065  0.7807521  0.7223016
##   0.0075  0.7590944  0.6946126
##   0.0085  0.7510045  0.6840994
##   0.0095  0.7461633  0.6783307
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 5e-04.
```

```r
plot(rpart_tune_fit)
```

![](index_files/figure-html/rparttune2-1.png)<!-- -->


By allowing increased complexity (i.e. decreasing `cp`), we have effectively added more variables into the model. Predicting on the validation set and taking a look at the confusion matrix shows that our model now includes variables that allow better prediction for Class D (as opposed to the simpler model which did not classify in D at all.)


```r
rpart_tune_pred <- predict(rpart_tune_fit, myval)
confusionMatrix(rpart_tune_pred, myval$classe)
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1064   32    7    6    3
##          B   30  660   26    8   25
##          C   10   37  620   29   18
##          D    5   17   20  584   20
##          E    7   13   11   16  655
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9133         
##                  95% CI : (0.9041, 0.922)
##     No Information Rate : 0.2845         
##     P-Value [Acc > NIR] : <2e-16         
##                                          
##                   Kappa : 0.8904         
##  Mcnemar's Test P-Value : 0.1313         
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9534   0.8696   0.9064   0.9082   0.9085
## Specificity            0.9829   0.9719   0.9710   0.9811   0.9853
## Pos Pred Value         0.9568   0.8812   0.8683   0.9040   0.9330
## Neg Pred Value         0.9815   0.9688   0.9801   0.9820   0.9795
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2712   0.1682   0.1580   0.1489   0.1670
## Detection Prevalence   0.2835   0.1909   0.1820   0.1647   0.1789
## Balanced Accuracy      0.9682   0.9207   0.9387   0.9447   0.9469
```

The more complex tree predicts on the validation set with over 91% accuracy, which again is in agreement with our estimate from repeated cross validation.

So in theory we could create an `rpart` tree that includes all of our variables, but rather than creating a single tree that is highly complex, which would increase variance and lead to overfitting, let's try a random forest model.

## Random Forest Model

I will use the `ranger` package, as it is faster than `randomForest`. I am using the same 5-fold cross validation as before.


```r
set.seed(21312)

ctrl <- trainControl(method = "cv", number = 5)

rf_fit <- train(classe ~ ., 
                data = mytrain, 
                method="ranger", 
                num.trees = 500,
                importance = 'impurity',
                trControl = ctrl)
```


```r
rf_fit
```

```
## Random Forest 
## 
## 15699 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 12558, 12561, 12560, 12558, 12559 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9925468  0.9905715
##   27    0.9919101  0.9897659
##   52    0.9859229  0.9821908
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 2.
```

The random forest model with 500 trees gives an accuracy estimate of 99.2% when the tuning parameter `mtry` is set to 2. We could fine tune this model and explore other values of `mtry`, but we will stick with 2, as the random forest model is fairly robust to variation in `mtry` over a small range for this project.


```r
rf_pred <- predict(rf_fit, myval)
confusionMatrix(rf_pred, myval$classe)
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1115    5    0    0    0
##          B    1  754    5    0    0
##          C    0    0  678   17    0
##          D    0    0    1  626    3
##          E    0    0    0    0  718
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9918          
##                  95% CI : (0.9885, 0.9944)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9897          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9991   0.9934   0.9912   0.9736   0.9958
## Specificity            0.9982   0.9981   0.9948   0.9988   1.0000
## Pos Pred Value         0.9955   0.9921   0.9755   0.9937   1.0000
## Neg Pred Value         0.9996   0.9984   0.9981   0.9948   0.9991
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2842   0.1922   0.1728   0.1596   0.1830
## Detection Prevalence   0.2855   0.1937   0.1772   0.1606   0.1830
## Balanced Accuracy      0.9987   0.9958   0.9930   0.9862   0.9979
```

The model predicts on `myval` with nearly 99.2% accuracy. 

This is the model we will use to generate our submissions for the test set predictions.

## Predicting on Test Set


```r
test_pred <- predict(rf_fit, test)
pred_df <- data.frame(id = test$problem_id, pred = as.character(test_pred))
```
