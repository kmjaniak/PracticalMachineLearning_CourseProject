---
title: "Practical Machine Learning - Course Project"
author: "Katie Martins"
date: "10/11/2017"
output: html_document
---

## Introduction

Popular devices like Jawbone Up, Nike FuelBand, and Fitbit allow for the collection of a large amount of data related to personal activity. The data collected from these devices is often used to quantify how much of a particular activity people are doing, but it is less often used to quantify how well they are doing it. 

In this project, we will build a machine learning model to perform quantitative activity recognition of weight lifting exercises. The dataset used in this project comes from http://groupware.les.inf.puc-rio.br/har. More information about the experiment can be found at that website.

Six participants were asked to perform one set of 10 repetitions of the unilateral dumbell biceps curl in 5 different manners:

* Class A: Exercise performed correctly
* Class B: Throwing the elbows to the front
* Class C: Lifting the dumbell only halfway
* Class D: Lowering the dumbell only halfway
* Class E: Throwing the hips to the front

The dataset contains observations comprised of measurements for each participant performing the exercise in each of the manners above, along with class labels identifying the manner in which the exercise was performed.

## Downloading Data

First, we load the libraries we will need for the analysis.

```{r libraries, message = FALSE, warning = FALSE}
library(dplyr)
library(ggplot2)
library(caret)
library(readr)
library(knitr)
library(Hmisc)
library(rattle)
```

Then we need to download the data and read it into R. 

```{r loaddata, message = FALSE, warning = FALSE, cache = TRUE}
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

download.file(trainURL, "training.csv")
download.file(testURL, "testing.csv")

training <- read_csv("training.csv", na = c("NA", ""))
testing <- read_csv("testing.csv", na = c("NA", ""))
```

## Data Processing

```{r names}
head(names(training), 15)
```

The first seven columns contain identifying information that will not be used to fit our model, so we will remove them from the data. 

```{r first7}
train <- training[, -(1:7)]
test <- testing[, -(1:7)]
```

```{r percent_na}
percent_na <- apply(train, 2, function(x) mean(is.na(x)))
na_cut <- cut2(percent_na, cuts = seq(0, 1, by = 0.1))
table(na_cut)
```
Of the 153 variables in our dataset, 100 of them contain over 90% missing values. Let's remove these variables from consideration. (These actually constitute aggregate statistics generated by the research team in their window-based approach to modeling the data, so, missing or not, it wouldn't make sense for us to include them in our model.)
```{r remove_na_cols}
na_cols <- which(percent_na > 0.9)
train <- train[,-na_cols]
test <- test[,-na_cols]
dim(train)
```
We are now down to 53 variables. Let's check whether there are any remaining missing values in these columns.
```{r morena}
nas <- sapply(train, function(x) sum(is.na(x)))
subset(nas, nas > 0)
```

Three of the remaining variables have one missing value. We will use median imputation to take care of those.

```{r impute}
preProc <- preProcess(train, method = "medianImpute")
train_imp <- predict(preProc, train)
```

We have already reduced the number of variables substantially by removing the columns that are predominantly NA, but let's check to see if any of the remaining columns are near zero variance.

```{r nzv, cache = TRUE}
nzv_cols <- nearZeroVar(train)
length(nzv_cols)
```

Because `length(nzv_cols)` is 0, none of our remaining variables can be excluded on the basis of near zero variance.

## Decision Tree Model

Now that we have cleaned up our data set and discarded some unnecessary variables, we can start fitting models. 

I am using 5-fold cross validation to estimate out of sample error. Additionally, for the purposes of this course project, I am holding out 20% of the data to use as a validation set. All of the data in `mytrain` will be used in fitting the model. The data in `myval` will only be used as new data for prediction.

```{r split}
set.seed(21312)
inTrain <- createDataPartition(train$classe, p = 0.8, list = FALSE)
mytrain <- train_imp[inTrain, ]
myval <- train_imp[-inTrain, ]
dim(mytrain)
dim(myval)
```

Let's build a simple decision tree using `rpart` with the default tuning parameters as a baseline model.

```{r rpartbaseline, cache = TRUE, message = FALSE, warning = FALSE}
set.seed(21312)
ctrl <- trainControl(method = "cv", number = 5)
rpart_fit <- train(classe ~ ., 
                   data = mytrain, 
                   method="rpart", 
                   trControl = ctrl)
rpart_fit
```

The accuracy estimate from this model is about 50.7%. We can use the `rattle` package to visualize our simple decision tree and see that the low accuracy is not surprising - only four variables were used in the final tree.

```{r rpartplot}
print(fancyRpartPlot(rpart_fit$finalModel))
```

Let's check how the baseline model does on the validation set and compare the result to the accuracy estimate from the cross validation procedure.

```{r rpartval}
rpart_pred <- predict(rpart_fit, myval)
confusionMatrix(rpart_pred, myval$classe)
```

The 50% accuracy on the validation set is in line with the out of sample error estimate from the cross validation procedure. Looking at the confusion matrix, we can see that this model does a decent job of predicting Class A, but does not predict that any of the observations are in Class D at all.

Before moving to more complex models, let's explore `rpart` further by considering tuning. The tuning parameter is the complexity parameter `cp`, and lower values of `cp` will result in more complex trees (i.e. more nodes = more variables included in the model). Let's try `cp = 0.01`.

```{r rparttune1, cache = FALSE, message = FALSE, warning = FALSE}
grid <- data.frame(cp = seq(0.01, 0.1, by = 0.01))
ctrl <- trainControl(method = "cv", number = 5)
rpart_cp_fit <- train(classe ~ ., 
                        data = mytrain, 
                        method="rpart", 
                        trControl = ctrl,
                        tuneGrid = grid)
rpart_cp_fit
```

This gives an accuracy estimate of about 74%. The tree includes many more variables than the tree with the default parameters.

```{r rpart_tune_rattle}
fancyRpartPlot(rpart_cp_fit$finalModel)
```

Just for fun, let's make the tree even more complex and look at the effect of `cp` on accuracy as we go lower.

```{r rparttune2, cache = TRUE, message = FALSE, warning = FALSE}
grid <- data.frame(cp = seq(0.0005, 0.01, by = 0.001))
ctrl <- trainControl(method = "cv", number = 5)
rpart_tune_fit <- train(classe ~ ., 
                        data = mytrain, 
                        method="rpart", 
                        trControl = ctrl,
                        tuneGrid = grid)
rpart_tune_fit
plot(rpart_tune_fit)
```


By allowing increased complexity (i.e. decreasing `cp`), we have effectively added more variables into the model. Predicting on the validation set and taking a look at the confusion matrix shows that our model now includes variables that allow better prediction for Class D (as opposed to the simpler model which did not classify in D at all.)

```{r rparttuneval}
rpart_tune_pred <- predict(rpart_tune_fit, myval)
confusionMatrix(rpart_tune_pred, myval$classe)
```

The more complex tree predicts on the validation set with over 91% accuracy, which again is in agreement with our estimate from repeated cross validation.

So in theory we could create an `rpart` tree that includes all of our variables, but rather than creating a single tree that is highly complex, which would increase variance and lead to overfitting, let's try a random forest model.

## Random Forest Model

I will use the `ranger` package, as it is faster than `randomForest`. I am using the same 5-fold cross validation as before.

```{r rfmodel, cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(21312)

ctrl <- trainControl(method = "cv", number = 5)

rf_fit <- train(classe ~ ., 
                data = mytrain, 
                method="ranger", 
                num.trees = 500,
                importance = 'impurity',
                trControl = ctrl)

```

```{r final, cache = TRUE, message = FALSE, warning = FALSE}
rf_fit
```

The random forest model with 500 trees gives an accuracy estimate of 99.2% when the tuning parameter `mtry` is set to 2. We could fine tune this model and explore other values of `mtry`, but we will stick with 2, as the random forest model is fairly robust to variation in `mtry` over a small range for this project.

```{r rfinalpred, message = FALSE, warning = FALSE}
rf_pred <- predict(rf_fit, myval)
confusionMatrix(rf_pred, myval$classe)
```

The model predicts on `myval` with nearly 99.2% accuracy. 

This is the model we will use to generate our submissions for the test set predictions.

## Predicting on Test Set

```{r predicttest}
test_pred <- predict(rf_fit, test)
pred_df <- data.frame(id = test$problem_id, pred = as.character(test_pred))
```